{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 15:39:41.469329: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-22 15:39:41.473843: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-22 15:39:41.535034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 15:39:42.745391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (55000, 28, 28)\n",
      "Shape of y_train: (55000,)\n",
      "Shape of x_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n",
      "Shape of x_val: (5000, 28, 28)\n",
      "Shape of y_val: (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"Shape of x_val:\", x_val.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 15:39:46.520684: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-22 15:39:46.521862: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 15:40:33.562311: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 172480000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8321 - loss: 0.5474 - val_accuracy: 0.9560 - val_loss: 0.1444\n",
      "Epoch 2/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9342 - loss: 0.2156 - val_accuracy: 0.9674 - val_loss: 0.1024\n",
      "Epoch 3/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9498 - loss: 0.1591 - val_accuracy: 0.9720 - val_loss: 0.0959\n",
      "Epoch 4/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9547 - loss: 0.1470 - val_accuracy: 0.9746 - val_loss: 0.0880\n",
      "Epoch 5/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9609 - loss: 0.1262 - val_accuracy: 0.9754 - val_loss: 0.0774\n",
      "Epoch 6/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9634 - loss: 0.1176 - val_accuracy: 0.9734 - val_loss: 0.0858\n",
      "Epoch 7/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9656 - loss: 0.1123 - val_accuracy: 0.9778 - val_loss: 0.0714\n",
      "Epoch 8/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.1008 - val_accuracy: 0.9796 - val_loss: 0.0695\n",
      "Epoch 9/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.0901 - val_accuracy: 0.9786 - val_loss: 0.0668\n",
      "Epoch 10/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0863 - val_accuracy: 0.9762 - val_loss: 0.0713\n",
      "Epoch 11/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.0848 - val_accuracy: 0.9802 - val_loss: 0.0685\n",
      "Epoch 12/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0766 - val_accuracy: 0.9814 - val_loss: 0.0666\n",
      "Epoch 13/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9744 - loss: 0.0814 - val_accuracy: 0.9806 - val_loss: 0.0698\n",
      "Epoch 14/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.0745 - val_accuracy: 0.9818 - val_loss: 0.0621\n",
      "Epoch 15/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9771 - loss: 0.0720 - val_accuracy: 0.9826 - val_loss: 0.0614\n",
      "Epoch 16/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.0677 - val_accuracy: 0.9796 - val_loss: 0.0664\n",
      "Epoch 17/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0686 - val_accuracy: 0.9814 - val_loss: 0.0628\n",
      "Epoch 18/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9784 - loss: 0.0654 - val_accuracy: 0.9828 - val_loss: 0.0619\n",
      "Epoch 19/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9792 - loss: 0.0671 - val_accuracy: 0.9822 - val_loss: 0.0651\n",
      "Epoch 20/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9802 - loss: 0.0631 - val_accuracy: 0.9830 - val_loss: 0.0648\n",
      "Epoch 21/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9814 - loss: 0.0592 - val_accuracy: 0.9832 - val_loss: 0.0654\n",
      "Epoch 22/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0595 - val_accuracy: 0.9820 - val_loss: 0.0657\n",
      "Epoch 23/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0571 - val_accuracy: 0.9830 - val_loss: 0.0633\n",
      "Epoch 24/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9819 - loss: 0.0557 - val_accuracy: 0.9818 - val_loss: 0.0658\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9757 - loss: 0.0790\n",
      "Baseline Accuracy: 0.9251\n",
      "Test Accuracy: 0.9803000092506409\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Building the Model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(28, 28)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Check if the model is a classification model\n",
    "is_classification_model = model.layers[-1].activation == tf.keras.activations.softmax\n",
    "\n",
    "# Step 3: Setting up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Step 4: Integrating TensorBoard\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "# Step 5: Baseline with Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000) \n",
    "logreg.fit(x_train.reshape(-1, 28*28), y_train)\n",
    "y_pred = logreg.predict(x_test.reshape(-1, 28*28))\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Training the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), callbacks=[early_stopping, tensorboard_callback], epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluating the Model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Printing the Results\n",
    "print(\"Baseline Accuracy:\", baseline_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-881f3f3e57445c31\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-881f3f3e57445c31\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une traduction en français du texte en markdown :\n",
    "\n",
    "```markdown\n",
    "Étape 1 : Chargement et prétraitement des données\n",
    "- La fonction `mnist.load_data()` est utilisée pour charger l'ensemble de données MNIST, qui contient des chiffres écrits à la main.\n",
    "- Les données sont divisées en ensembles d'entraînement et de test, avec `x_train` et `y_train` représentant les images d'entrée et leurs étiquettes correspondantes pour l'entraînement, et `x_test` et `y_test` représentant les images d'entrée et leurs étiquettes correspondantes pour les tests.\n",
    "- Les valeurs des pixels des images sont normalisées en les divisant par 255.0, ce qui met les valeurs à l'échelle entre 0 et 1.\n",
    "\n",
    "Étape 2 : Construction du modèle\n",
    "- Le modèle `Sequential` est créé en utilisant `model = Sequential()`.\n",
    "- La forme d'entrée du modèle est définie comme `(28, 28)` en utilisant `Flatten(input_shape=(28, 28))`, ce qui aplatit les images d'entrée 2D en un tableau 1D.\n",
    "- Deux couches entièrement connectées avec 128 et 64 unités, respectivement, sont ajoutées en utilisant des couches `Dense` avec la fonction d'activation `relu`.\n",
    "- Des couches `BatchNormalization` sont ajoutées après chaque couche `Dense` pour normaliser les activations.\n",
    "- Des couches `Dropout` avec un taux de désactivation de 0.2 sont ajoutées après chaque couche `BatchNormalization` pour éviter le surajustement.\n",
    "- La couche de sortie finale avec 10 unités et la fonction d'activation `softmax` est ajoutée en utilisant `Dense(10, activation='softmax')`, car il s'agit d'un problème de classification multi-classes avec 10 classes.\n",
    "\n",
    "Étape 3 : Configuration de l'arrêt précoce\n",
    "- Un rappel `EarlyStopping` est créé avec le paramètre `monitor` défini sur `'val_accuracy'`, ce qui signifie qu'il surveillera la précision de validation.\n",
    "- Le paramètre `patience` est défini sur 3, ce qui signifie que l'entraînement s'arrêtera si la précision de validation n'améliore pas pendant 3 époques consécutives.\n",
    "- Le paramètre `restore_best_weights` est défini sur `True`, ce qui signifie que les poids du modèle seront restaurés aux meilleurs poids trouvés pendant l'entraînement.\n",
    "\n",
    "Étape 4 : Intégration de TensorBoard\n",
    "- Un rappel `TensorBoard` est créé avec le paramètre `log_dir` défini sur `'./logs'`, ce qui spécifie le répertoire où les journaux seront enregistrés.\n",
    "- Le paramètre `histogram_freq` est défini sur 1, ce qui signifie que les histogrammes des poids et des biais du modèle seront calculés et enregistrés à chaque époque.\n",
    "\n",
    "Étape 5 : Baseline avec la régression logistique\n",
    "- Un modèle `LogisticRegression` est créé et entraîné en utilisant les données d'entraînement.\n",
    "- Les tableaux `x_train` et `x_test` sont remodelés en tableaux 2D en utilisant `reshape(-1, 28*28)` pour correspondre à la forme d'entrée attendue par `LogisticRegression`.\n",
    "- La méthode `predict` est utilisée pour effectuer des prédictions sur les données de test, et la précision des prédictions est calculée à l'aide de `accuracy_score` et stockée dans la variable `baseline_accuracy`.\n",
    "\n",
    "Entraînement du modèle\n",
    "- Le modèle est compilé avec l'optimiseur `adam` et la fonction de perte `sparse_categorical_crossentropy`, qui convient aux problèmes de classification multi-classes.\n",
    "- La méthode `fit` est appelée pour entraîner le modèle en utilisant les données d'entraînement.\n",
    "- Les données de validation `(x_val, y_val)` sont utilisées pour surveiller la précision de validation pendant l'entraînement.\n",
    "- Les rappels `early_stopping` et `tensorboard_callback` sont transmis au paramètre `callbacks` pour activer l'arrêt précoce et l'intégration de TensorBoard.\n",
    "- L'entraînement est effectué pendant 10 époques.\n",
    "\n",
    "Évaluation du modèle\n",
    "- La méthode `evaluate` est appelée pour évaluer le modèle entraîné sur les données de test.\n",
    "- La perte de test et la précision de test sont stockées dans les variables `test_loss` et `test_accuracy`, respectivement.\n",
    "\n",
    "Affichage des résultats\n",
    "- La précision de référence et la précision de test sont affichées à l'aide de la fonction `print`.\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
