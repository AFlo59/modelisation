{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade tensorflow\n",
    "# %pip uninstall tensorboard -y\n",
    "# %pip install tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (55000, 28, 28)\n",
      "Shape of y_train: (55000,)\n",
      "Shape of x_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n",
      "Shape of x_val: (5000, 28, 28)\n",
      "Shape of y_val: (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"Shape of x_val:\", x_val.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 16:30:46.187523: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 172480000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8351 - loss: 0.5438 - val_accuracy: 0.9532 - val_loss: 0.1505\n",
      "Epoch 2/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9377 - loss: 0.2094 - val_accuracy: 0.9702 - val_loss: 0.1077\n",
      "Epoch 3/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9497 - loss: 0.1676 - val_accuracy: 0.9692 - val_loss: 0.0994\n",
      "Epoch 4/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9560 - loss: 0.1434 - val_accuracy: 0.9740 - val_loss: 0.0876\n",
      "Epoch 5/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.1334 - val_accuracy: 0.9722 - val_loss: 0.0835\n",
      "Epoch 6/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9617 - loss: 0.1237 - val_accuracy: 0.9730 - val_loss: 0.0859\n",
      "Epoch 7/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.1082 - val_accuracy: 0.9744 - val_loss: 0.0803\n",
      "Epoch 8/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9670 - loss: 0.1042 - val_accuracy: 0.9768 - val_loss: 0.0734\n",
      "Epoch 9/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.0974 - val_accuracy: 0.9800 - val_loss: 0.0700\n",
      "Epoch 10/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.0944 - val_accuracy: 0.9808 - val_loss: 0.0665\n",
      "Epoch 11/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0829 - val_accuracy: 0.9802 - val_loss: 0.0642\n",
      "Epoch 12/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0875 - val_accuracy: 0.9798 - val_loss: 0.0701\n",
      "Epoch 13/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9752 - loss: 0.0770 - val_accuracy: 0.9824 - val_loss: 0.0612\n",
      "Epoch 14/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.0774 - val_accuracy: 0.9810 - val_loss: 0.0663\n",
      "Epoch 15/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9756 - loss: 0.0754 - val_accuracy: 0.9826 - val_loss: 0.0608\n",
      "Epoch 16/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9792 - loss: 0.0653 - val_accuracy: 0.9842 - val_loss: 0.0587\n",
      "Epoch 17/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9774 - loss: 0.0680 - val_accuracy: 0.9848 - val_loss: 0.0554\n",
      "Epoch 18/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9797 - loss: 0.0666 - val_accuracy: 0.9834 - val_loss: 0.0587\n",
      "Epoch 19/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9810 - loss: 0.0584 - val_accuracy: 0.9810 - val_loss: 0.0583\n",
      "Epoch 20/100\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9802 - loss: 0.0594 - val_accuracy: 0.9832 - val_loss: 0.0562\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9786 - loss: 0.0705\n",
      "Baseline Accuracy: 0.9251\n",
      "Test Accuracy: 0.9815000295639038\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Building the Model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(28, 28)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Check if the model is a classification model\n",
    "is_classification_model = model.layers[-1].activation == tf.keras.activations.softmax\n",
    "\n",
    "# Step 3: Setting up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Step 4: Integrating TensorBoard\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "# Step 5: Baseline with Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000) \n",
    "logreg.fit(x_train.reshape(-1, 28*28), y_train)\n",
    "y_pred = logreg.predict(x_test.reshape(-1, 28*28))\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Training the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), callbacks=[early_stopping, tensorboard_callback], epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluating the Model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Printing the Results\n",
    "print(\"Baseline Accuracy:\", baseline_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 893392), started 0:25:55 ago. (Use '!kill 893392' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1804c3c829af9a3a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1804c3c829af9a3a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# Reload the TensorBoard extension\n",
    "%reload_ext tensorboard\n",
    "\n",
    "# Start TensorBoard within the notebook using magics function and display scalars\n",
    "# %tensorboard --logdir logs --samples_per_plugin scalars\n",
    "# Start TensorBoard within the notebook using magics function\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une traduction en français du texte en markdown :\n",
    "\n",
    "```markdown\n",
    "Étape 1 : Chargement et prétraitement des données\n",
    "- La fonction `mnist.load_data()` est utilisée pour charger l'ensemble de données MNIST, qui contient des chiffres écrits à la main.\n",
    "- Les données sont divisées en ensembles d'entraînement et de test, avec `x_train` et `y_train` représentant les images d'entrée et leurs étiquettes correspondantes pour l'entraînement, et `x_test` et `y_test` représentant les images d'entrée et leurs étiquettes correspondantes pour les tests.\n",
    "- Les valeurs des pixels des images sont normalisées en les divisant par 255.0, ce qui met les valeurs à l'échelle entre 0 et 1.\n",
    "\n",
    "Étape 2 : Construction du modèle\n",
    "- Le modèle `Sequential` est créé en utilisant `model = Sequential()`.\n",
    "- La forme d'entrée du modèle est définie comme `(28, 28)` en utilisant `Flatten(input_shape=(28, 28))`, ce qui aplatit les images d'entrée 2D en un tableau 1D.\n",
    "- Deux couches entièrement connectées avec 128 et 64 unités, respectivement, sont ajoutées en utilisant des couches `Dense` avec la fonction d'activation `relu`.\n",
    "- Des couches `BatchNormalization` sont ajoutées après chaque couche `Dense` pour normaliser les activations.\n",
    "- Des couches `Dropout` avec un taux de désactivation de 0.2 sont ajoutées après chaque couche `BatchNormalization` pour éviter le surajustement.\n",
    "- La couche de sortie finale avec 10 unités et la fonction d'activation `softmax` est ajoutée en utilisant `Dense(10, activation='softmax')`, car il s'agit d'un problème de classification multi-classes avec 10 classes.\n",
    "\n",
    "Étape 3 : Configuration de l'arrêt précoce\n",
    "- Un rappel `EarlyStopping` est créé avec le paramètre `monitor` défini sur `'val_accuracy'`, ce qui signifie qu'il surveillera la précision de validation.\n",
    "- Le paramètre `patience` est défini sur 3, ce qui signifie que l'entraînement s'arrêtera si la précision de validation n'améliore pas pendant 3 époques consécutives.\n",
    "- Le paramètre `restore_best_weights` est défini sur `True`, ce qui signifie que les poids du modèle seront restaurés aux meilleurs poids trouvés pendant l'entraînement.\n",
    "\n",
    "Étape 4 : Intégration de TensorBoard\n",
    "- Un rappel `TensorBoard` est créé avec le paramètre `log_dir` défini sur `'./logs'`, ce qui spécifie le répertoire où les journaux seront enregistrés.\n",
    "- Le paramètre `histogram_freq` est défini sur 1, ce qui signifie que les histogrammes des poids et des biais du modèle seront calculés et enregistrés à chaque époque.\n",
    "\n",
    "Étape 5 : Baseline avec la régression logistique\n",
    "- Un modèle `LogisticRegression` est créé et entraîné en utilisant les données d'entraînement.\n",
    "- Les tableaux `x_train` et `x_test` sont remodelés en tableaux 2D en utilisant `reshape(-1, 28*28)` pour correspondre à la forme d'entrée attendue par `LogisticRegression`.\n",
    "- La méthode `predict` est utilisée pour effectuer des prédictions sur les données de test, et la précision des prédictions est calculée à l'aide de `accuracy_score` et stockée dans la variable `baseline_accuracy`.\n",
    "\n",
    "Entraînement du modèle\n",
    "- Le modèle est compilé avec l'optimiseur `adam` et la fonction de perte `sparse_categorical_crossentropy`, qui convient aux problèmes de classification multi-classes.\n",
    "- La méthode `fit` est appelée pour entraîner le modèle en utilisant les données d'entraînement.\n",
    "- Les données de validation `(x_val, y_val)` sont utilisées pour surveiller la précision de validation pendant l'entraînement.\n",
    "- Les rappels `early_stopping` et `tensorboard_callback` sont transmis au paramètre `callbacks` pour activer l'arrêt précoce et l'intégration de TensorBoard.\n",
    "- L'entraînement est effectué pendant 10 époques.\n",
    "\n",
    "Évaluation du modèle\n",
    "- La méthode `evaluate` est appelée pour évaluer le modèle entraîné sur les données de test.\n",
    "- La perte de test et la précision de test sont stockées dans les variables `test_loss` et `test_accuracy`, respectivement.\n",
    "\n",
    "Affichage des résultats\n",
    "- La précision de référence et la précision de test sont affichées à l'aide de la fonction `print`.\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
